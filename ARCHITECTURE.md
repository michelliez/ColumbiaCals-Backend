# ColumbiaCals Backend - Multi-University Architecture

## ğŸ“ New Structure

```
ColumbiaCals-Backend/
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ shared/
â”‚   â”‚   â””â”€â”€ __init__.py          # Shared utilities (time checks, etc.)
â”‚   â”œâ”€â”€ columbia/
â”‚   â”‚   â””â”€â”€ scraper.py           # Columbia & Barnard scraper
â”‚   â””â”€â”€ cornell/
â”‚       â””â”€â”€ scraper.py           # Cornell scraper (placeholder)
â”œâ”€â”€ run_all_scrapers.py          # Unified runner (combines all scrapers)
â”œâ”€â”€ server.py                    # Flask API server
â”œâ”€â”€ nutrition_api.py             # Nutrition data enrichment
â”œâ”€â”€ menu_data.json               # Combined output from all scrapers
â”œâ”€â”€ requirements.txt             # Dependencies
â””â”€â”€ README.md                    # This file
```

## ğŸ« University Support

### Columbia University âœ…
- **Status**: Fully implemented
- **Coverage**: Columbia dining halls + Barnard dining halls
- **Scraper**: `scrapers/columbia/scraper.py`
- **Locations**: John Jay, Ferris Booth, Grace Dodge, Faculty House, Robert F. Smith, Blue Java cafes, etc.

### Cornell University ğŸš§
- **Status**: Placeholder (ready for implementation)
- **Scraper**: `scrapers/cornell/scraper.py`
- **Next Steps**: 
  1. Find Cornell's dining API or website
  2. Implement Cornell-specific scraping logic
  3. Add actual dining hall locations

## ğŸš€ Usage

### Run all scrapers (Recommended)
```bash
python3 run_all_scrapers.py
```

### Run specific university scraper
```bash
python3 scrapers/columbia/scraper.py    # Columbia only
python3 scrapers/cornell/scraper.py     # Cornell only (not yet implemented)
```

### Start the API server
```bash
python3 server.py
```

## ğŸ“Š Output Format

All scrapers output to `menu_data.json` with this structure:

```json
[
  {
    "name": "John Jay Dining Hall",
    "university": "columbia",
    "status": "open",
    "meals": [
      {
        "meal_type": "Lunch",
        "time": "11:30 AM - 2:00 PM",
        "stations": [
          {
            "station": "Main Station",
            "items": [
              {
                "name": "Grilled Chicken",
                "description": "...",
                "dietary_prefs": ["halal"],
                "allergens": ["gluten"]
              }
            ]
          }
        ]
      }
    ],
    "scraped_at": "2026-02-01T12:34:56.789123"
  }
]
```

## ğŸ”§ Adding a New University

1. Create a new folder: `scrapers/[university_name]/`
2. Create `scrapers/[university_name]/scraper.py` with:
   - Function `scrape_[university_name]()` that returns list of dining hall data
   - Import from `shared` for utilities
3. Update `run_all_scrapers.py`:
   ```python
   from [university_name].scraper import scrape_[university_name]
   
   # In run_all_scrapers():
   [university_name]_results = scrape_[university_name]()
   all_results.extend([university_name]_results)
   ```

## ğŸ“ Requirements

See `requirements.txt`:
- Flask
- Flask-CORS
- Requests
- BeautifulSoup4
- Schedule

## ğŸŒ API Endpoints

The Flask server (`server.py`) provides:

- `GET /api/dining` - Get all dining data
- `GET /api/dining?university=columbia` - Get Columbia-specific data
- `GET /api/dining?university=cornell` - Get Cornell-specific data

## ğŸ“… Scheduled Updates

The server includes a background scheduler that runs:
1. All scrapers (via `run_all_scrapers.py`)
2. Nutrition API enrichment (via `nutrition_api.py`)

Runs automatically at scheduled intervals or can be manually triggered.
